# 爬虫基础



近期因为某些奇怪的需求，需要对爬虫进行学习。以此文章作为记录，方便日后需要时查看。



所谓爬虫，实际上就是通过python中的一些库来编写自动化脚本。让程序伪装成浏览器向某网站的服务器发送请求，并获得返回信息，对返回信息进行加工处理，选取我们所需要的内容保存。



爬虫时需要使用到的一些库：urllib, re, bs4等等



首先要了解需要想服务器发送什么请求

其中较为重要的是cookie，该条保存了用户的登录信息。当你获得了cookie并将其加入请求头部向服务器发送时，服务器会根据cookie内容识别是哪一个账户，再根据识别内容返回response。

```python
import urllib.request, urllib.error
import re
from bs4 import BeautifulSoup

url = "www.teapot.tom"


headers = {
"Cookie": "I'm a teapot!"
}
req = urllib.request.Request(url=url, headers=headers)
response = urllib.request.urlopen(req)
# print(response.read().decode("utf-8"))
html = response.read().decode("utf-8")
```





### 正则表达式

略



#### 0x1从页面上提取到每次尝试的链接

```python
import urllib.request, urllib.error
import re
from bs4 import BeautifulSoup

def getlink():
    url = "I'm a teapot!"

    findLink = re.compile(r'<a href="(.*?)">')
    headers = {
    "Cookie": "I'm a teapot!"
    }
    req = urllib.request.Request(url=url, headers=headers)
    response = urllib.request.urlopen(req)
    # print(response.read().decode("utf-8"))
    html = response.read().decode("utf-8")
    # print(html)
    bs = BeautifulSoup(html, "html.parser")
    for item in bs.find_all("table", class_="attachments"):
        item = str(item)
        link = re.findall(findLink, item)

    for i in range(len(link)):
        link[i] = link[i].replace("amp;", "")
    for i in link:
         print(i)
    return link
```





#### 0x2根据上文提取到的链接将题库爬下来

```python
import urllib.request, urllib.error
import re
from bs4 import BeautifulSoup
import timupaqu2
num = 0
save_path = "?"

baseurl = "https://wlkc.ouc.edu.cn"
link = timupaqu2.getlink()
#print(link)
headers = {
"Cookie": "I'm a teapot!"
}
findti = re.compile(r'([^\x00-\xff][^\x00-\x2f\x3a-\xff]*)')
final_list = []
for i in link:
    url = baseurl + i
    print(url)
    req = urllib.request.Request(url=url, headers=headers)
    response = urllib.request.urlopen(req)
    # print(response.read().decode("utf-8"))
    html = response.read().decode("utf-8")
    # print(html)
    bs = BeautifulSoup(html, "html5lib")
    item1 = ""
    for item in bs.find_all("div", class_="details"):
        item = str(item)
        # print(item)
        ti = re.findall(findti, item)
        if ti[0] in final_list:
            continue
        for item2 in ti:
            if item2 != "正确" and item2 != "正确答案：":
                final_list.append(item2)
        final_list.append('')
        num = num + 1

for item in final_list:
    print(item)
print(num)

with open(save_path, "w+") as f:
    for item in final_list:
        f.write(item+'\n')

```

