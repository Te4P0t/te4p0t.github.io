---
layout: post
title: 爬虫基础
subheading: 爬点题库
author: Te4P0t
categories: 无
tags: Python
---



# 爬虫基础





所谓爬虫，实际上就是通过python中的一些库来编写自动化脚本。让程序伪装成浏览器向某网站的服务器发送请求，并获得返回信息，对返回信息进行加工处理，选取我们所需要的内容保存。



爬虫时需要使用到的一些库：urllib, re, bs4等等



首先要了解需要向服务器发送什么请求

其中较为重要的是cookie，什么是cookie?

Cookie 是一些数据, 存储于你电脑上的文本文件中。

当 web 服务器向浏览器发送 web 页面时，在连接关闭后，服务端不会记录用户的信息。

Cookie 的作用就是用于解决 "如何记录客户端的用户信息":

- 当用户访问 web 页面时，他的名字可以记录在 cookie 中。
- 在用户下一次访问该页面时，可以在 cookie 中读取用户访问记录。

```python
import urllib.request, urllib.error
import re
from bs4 import BeautifulSoup

url = "www.teapot.tom"


headers = {
"Cookie": "I'm a teapot!"
}
req = urllib.request.Request(url=url, headers=headers)
response = urllib.request.urlopen(req)
# print(response.read().decode("utf-8"))
html = response.read().decode("utf-8")
```





### 正则表达式

略



#### 0x1从页面上提取到每次尝试的链接

```python
import urllib.request, urllib.error
import re
from bs4 import BeautifulSoup

def getlink():
    url = "I'm a teapot!"

    findLink = re.compile(r'<a href="(.*?)">')
    headers = {
    "Cookie": "I'm a teapot!"
    }
    req = urllib.request.Request(url=url, headers=headers)
    response = urllib.request.urlopen(req)
    # print(response.read().decode("utf-8"))
    html = response.read().decode("utf-8")
    # print(html)
    bs = BeautifulSoup(html, "html.parser")
    for item in bs.find_all("table", class_="attachments"):
        item = str(item)
        link = re.findall(findLink, item)

    for i in range(len(link)):
        link[i] = link[i].replace("amp;", "")
    for i in link:
         print(i)
    return link
```





#### 0x2根据上文提取到的链接将题库爬下来

```python
import urllib.request, urllib.error
import re
from bs4 import BeautifulSoup
import timupaqu2
num = 0
save_path = "?"

baseurl = "https://wlkc.ouc.edu.cn"
link = timupaqu2.getlink()
#print(link)
headers = {
"Cookie": "I'm a teapot!"
}
findti = re.compile(r'([^\x00-\xff][^\x00-\x2f\x3a-\xff]*)')
final_list = []
for i in link:
    url = baseurl + i
    print(url)
    req = urllib.request.Request(url=url, headers=headers)
    response = urllib.request.urlopen(req)
    # print(response.read().decode("utf-8"))
    html = response.read().decode("utf-8")
    # print(html)
    bs = BeautifulSoup(html, "html5lib")
    item1 = ""
    for item in bs.find_all("div", class_="details"):
        item = str(item)
        # print(item)
        ti = re.findall(findti, item)
        if ti[0] in final_list:
            continue
        for item2 in ti:
            if item2 != "正确" and item2 != "正确答案：":
                final_list.append(item2)
        final_list.append('')
        num = num + 1

for item in final_list:
    print(item)
print(num)

with open(save_path, "w+") as f:
    for item in final_list:
        f.write(item+'\n')

```

