---
layout: post
title: 支持向量机——SMO算法实现
subheading: SMO
author: te4p0t
categories: Machine-Learning
tags: Machine-Learning Computer-Vision
---

# 支持向量机——SMO算法实现

序列最小优化算法（Sequential minimal optimization）

## 推导过程

### 坐标下降

![image-20220603174753792](https://te4p0t.github.io/assets/images/typora-user-images/202206141104332.png)

传统的梯度下降在求梯度时，需要对每一个参数去求偏导。但有时计算起来很复杂，所以有一个替代方案是坐标下降。

坐标下降每次固定其他维度，只对一个维度进行优化。因为在每次坐标下降的过程中，只需对一个变量求偏导，因此计算的复杂度大大降低。对于一个复杂的优化问题，分解为了许多小问题来解决。

### SMO更新策略

SVM对偶问题需要解决的
$$
最小化：\theta(\alpha) =  \frac{1}{2}\sum\limits^N_{i=1}\sum\limits^N_{j=1}\alpha_i\alpha_jy_iy_j\kappa(X_i,X_j) - \sum\limits^N_{i=1}\alpha_i\\
限制条件： 0 \le \alpha_i \le C \\
\qquad\qquad\sum\limits^N_{i=1}\alpha_iy_i=0\tag 1
$$
进行坐标梯度下降时需要同时更新至少两个变量

因为对单一变量的更新会破坏第二个约束条件

不失一般性地我们选择优化$\alpha_1$和$\alpha_2$这两个变量，固定其他变量。
$$
L(\alpha_1, \alpha_2) = \frac{1}{2}[\alpha_1y_1\alpha_1y_1\kappa(x_1,x_1) + 2\alpha_1y_1\alpha_2y_2\kappa(x_1, x_2) + \alpha_2y_2\alpha_2y_2\kappa(x_2,x_2) + 2\sum\limits^N_{j=3}\alpha_1y_1\alpha_jy_j\kappa(x_1,x_j) + 2\sum\limits^N_{j=3}\alpha_2y_2\alpha_jy_j\kappa(x_2,x_j)+\sum\limits^N_{i=3}\sum\limits^N_{j=3}\alpha_i\alpha_jy_iy_j\kappa(x_i,x_j)] - [\alpha_1 + \alpha_2 + \sum\limits^N_{j=3}\alpha_j]
$$
其中有些常数对求导结果没有影响，在这里略去，同时将$\kappa(x_i, x_j)$简写为$K_{ij}$的形式，还有$y_i^2=1$，代入整理后的式子如下
$$
L(\alpha_1, \alpha_2) = \frac{1}{2}[\alpha_1^2K_{11} + 2\alpha_1y_1\alpha_2y_2K_{12} + \alpha_2^2K_{22} + 2\sum\limits^N_{j=3}\alpha_1y_1\alpha_jy_jK_{1j} + 2\sum\limits^N_{j=3}\alpha_2y_2\alpha_jy_jK_{2j}] - [\alpha_1 + \alpha_2] \tag 2
$$
又因为约束条件2
$$
\sum\limits^N_{i=1}\alpha_iy_i=\alpha_1y_1+\alpha_2y_2+\sum\limits^N_{i=3}\alpha_iy_i =  0 \\
$$
设$\sum\limits^N_{i=3}\alpha_iy_i = \xi$，可得
$$
\alpha_1 = y_1(\xi-\alpha_2y_2) \tag 3
$$
将（3）式代入（2）式可得
$$
L(\alpha_2) = \frac{1}{2}[(\xi-\alpha_2y_2)^2K_{11}+2(\xi-\alpha_2y_2)\alpha_2y_2K_{12} +\alpha_2^2K_{22} + 2\sum\limits^N_{j=3}(\xi-\alpha_2y_2)\alpha_jy_jK_{1j}+2\sum\limits^N_{j=3}\alpha_2y_2\alpha_jy_jK_{2j}]-[y_1(\xi-\alpha_2y_2)+\alpha_2] \tag 4
$$
对（4）式求导，可得
$$
L'(\alpha_2) = \frac{1}{2}[-2y_2(\xi-\alpha_2y_2)K_{11} + 2\xi y_2K_{12}-4\alpha_2K_{12}+2\alpha_2K_{22}-2\sum\limits^N_{i=3}y_2\alpha_iy_iK_{1i}+2\sum\limits^N_{i=3}y_2\alpha_iy_iK_{2i}] + y_1y_2-1
$$
整理可得
$$
L'(\alpha_2) = y_1y_2-1-y_2\xi K_{11}+\alpha_2K_{11}+\xi y_2K_{12}-2\alpha_2K_{12}+\alpha_2K_{22}-\sum\limits^N_{i=3}y_2\alpha_iy_iK_{1i}+\sum\limits^N_{i=3}y_2\alpha_iy_iK_{2i} \tag 5
$$
令（5）式为0，化简可得
$$
\alpha_2(K_{11}+K_{22}-2K_{12}) = y_2[y_2 - y_1 + \xi K_{11}-\xi K_{12}+\sum\limits^N_{i=3}\alpha_iy_iK_{1i}-\sum\limits^N_{i=3}\alpha_jy_jK{2j}] \tag 6
$$
在这里，因为SMO是一个迭代的算法，所以对于更新前的$\alpha_1^{old}$和$\alpha_2^{old}$有
$$
\alpha_1^{old}y_1+\alpha_2^{old}y_2 = \xi \tag 7
$$
同时在之前SVM推导过程中，令$\displaystyle \frac{\partial L}{\partial W} = 0$得到$W=\sum\limits^N_{i=1}\alpha_iy_i\phi(X_i)$

又$f(x) = W^TX+b$

所以
$$
f(x_1) = \alpha_1y_1K_{11} + \alpha_2y_2K_{12} + \sum\limits^N_{i=3}\alpha_iy_iK_{1i} + b \\
f(x_2) = \alpha_1y_1K_{12} + \alpha_2y_2K_{22} + \sum\limits^N_{i=3}\alpha_iy_iK_{2i} + b
$$
化简上式
$$
\sum\limits^N_{i=3}\alpha_iy_iK_{1i} = f(x_1) -\alpha_1y_1K_{11}-\alpha_2y_2K_{12}-b \\
\sum\limits^N_{i=3}\alpha_iy_iK_{2i} = f(x_2) -\alpha_1y_1K_{12}-\alpha_2y_2K_{22}-b \tag 8
$$
将（7）、（8）代入（6）式可得
$$
\alpha_2(K_{11}+K_{22}-2K_{12}) = y_2[y_2-y_1+\alpha_1^{old}y_1K_{11}+\alpha_2^{old}y_2K_{11}-\alpha_1^{old}y_1K_{12}-\alpha_2^{old}y_2K_{12}+f(x_1)-\alpha_2^{old}y_1K_{11}-\alpha_2^{old}y_2K_{12}-b-f(x_2)+\alpha_1^{old}y_1K_{12}+\alpha_2^{old}y_2K_{22}+b]
$$
整理上式，合并同类项得
$$
\alpha_2(K_{11}+K_{22}-2K_{12}) = y_2[(f(x_1)-y_1)-(f(x_2)-y_2)+\alpha_2^{old}y_2(K_{11}+K_{22}-2K_{12})] \tag 9
$$
令$E_1=f(x_1)-y_1,E_2=f(x_2)-y_2,\eta =K_{11}+K_{22}-2K_{12}$，则可以得到$\alpha_2$迭代更新的一个很简明的式子
$$
\alpha_2^{new} = \alpha_2^{old} + \frac{y_2(E_1-E_2)}{\eta} \tag {10}
$$

### 剪辑最优解

上述递推公式虽然简单，但是要注意到在SVM的对偶问题中，$\alpha$有着限制条件，因此在更新过程中也要符合限制条件。
$$
0\le \alpha \le C \\
\alpha_1y_i+\alpha_2y_2 = \xi
$$
对第二个限制条件进行分类讨论

设$\alpha$的上下界分别为$L$和$H$

1. 当$y_1 \ne y_2$时，有$\alpha_1 - \alpha_2 = k$，根据$y_1$和$y_2$的不同取值，$k$可能为$-\xi$或$\xi$。

   因为$\alpha_1 \sube [0, C]$且$\alpha_2= \alpha_1 - k $

   所以$\alpha_2 \sube [-k, C-k] = [\alpha_2^{old}-\alpha_1^{old}, C+\alpha_2^{old}-\alpha_1^{old}]$

   又$\alpha_2 \sube [0, C]$

   两者取交集$L=\max(0, \alpha_2^{old}-\alpha_1^{old})$，$H = \min(C, C+\alpha_2^{old}-\alpha_1^{old})$

2. 当$y_1 = y_2$时，$\alpha_1 + \alpha_2 = k$

   因为$\alpha_1 \sube [0, C]$且$\alpha_2= k-\alpha_1$

   所以$\alpha_2 \sube [k-C, k] = [\alpha_2^{old}+\alpha_1^{old}-C,\alpha_2^{old}-\alpha_1^{old}]$

   又$\alpha_2 \sube [0, C]$

   两者取交集$L=\max(0,\alpha_1^{okd} +\alpha_2^{old}-C)$，$H = \min(C,\alpha_2^{old}+\alpha_1^{old})$

剪辑后的解
$$
\alpha_2^{final} = 
\begin{cases}
H, & \alpha_2^{new} \gt H \\
\alpha_2^{new}, &L \le \alpha_2^{new}\le H \\
L, & \alpha_2^{new} \lt L
\end{cases}
$$

### $\alpha_1$和$\alpha_2$的选择

因为满足强对偶定理的充要条件是KKT条件，而满足强对偶定理的对偶问题最优解才和原问题相同，因此SMO的目标是使所有变量的解都满足KKT条件。

因此选择的两个$\alpha$中至少有一个不满足KKT条件。直观来看，KKT条件违反的程度越大，则变量更新后可能导致的目标函数值越接近于预期。因此，SMO先选择违背KKT条件程度最大的变量。

**KKT条件**

列出这个问题的KKT及其他条件
$$
\begin{aligned}
&\alpha_i \ge 0,\qquad \beta_i \ge 0 \quad (对偶问题可行)\\
&1+\xi_i-y_if(x_i) \le 0 \\
&\xi_i \le 0,\qquad \beta_i\xi_i=0 \\
&\alpha_i(1+\xi_i -y_if(x_i)) = 0
\end{aligned}
$$
以及等式$C = \beta_i +\alpha_i$

针对$\alpha_i$的不同取值分类讨论

1. $\alpha_i = 0$， 则$\beta_i = C  \Rarr \xi_i = 0$，所以$1\le y_if(x_i)$，该点符合分割条件。
2. $0\lt\alpha_i\lt C$，则$1+\xi_i-y_if(x_i)=0$，同时$\beta_i \ne 0 \Rarr \xi_i=0$，所以$1 = y_if(x_i)$，该点是支持向量。
3. $\alpha_i=C$，则$1+\xi_i-y_if(x_i)=0$，又$\xi_i \le 0$，所以$1-y_if(x_i)\ge0$，所以$1\ge y_if(x_i)$ ，该点越过了分割边界。

可以用此条件来判断某个变量$\alpha$是否符合KKT条件。

第二个变量的选择：使目标函数有足够大的变化（即对应$|E_1-E_2|$最大，即$E_1$与$E_2$符号相反，差异最大）

### 计算b和$E_i$

由KKT条件可知，当$0 \lt \alpha_1^{new} \lt C$，则有
$$
y_1f(x_1) = 1 \\
\Rarr \sum\limits^N_{i=1}\alpha_iy_iK_{1i} + b = y_1 \\
\Rarr b_1^{new} = y_1 - \sum\limits^N_{i=3}\alpha_iy_iK_{1i} -\alpha_1^{new}y_1K_{11}-\alpha_2^{new} y_2K_{12}
$$
求$E_1$
$$
E_1  = f(x_1)-y_1 = \sum\limits^N_{i=3}\alpha_iy_iK_{1i} + \alpha_1^{old}y_1K_{11} + \alpha_2^{old}y_2K_{12} + b^{old} - y_1
$$
将$E_1$代入$b_1^{new}$式子中
$$
b_1^{new} = -E_1-y_1K_{11}(\alpha_1^{new}-\alpha_1^{old})-y_2K_{12}(\alpha_2^{new}-\alpha_2^{old})+b^{old}
$$
同理
$$
b_2^{new} =-E_2-y_1K_{12}(\alpha_1^{new}-\alpha_1^{old})-y_2K_{22}(\alpha_2^{new}-\alpha_2^{old})+b^{old}
$$

## 实现

### 加载数据集

```python
import numpy as np
import matplotlib.pyplot as plt

data = np.loadtxt('./data.txt')
# print(data)
train_data = data[:,:2]
train_label = data[:,-1]
train_label = np.where(train_label != 0, train_label, -1)
print(len(train_data))
```

### 线性核、计算f(x)和E(x)

```python
class SVM(object):
    def __init__(self, iters, tol, kernel, C, train_data, train_label):
        self.iters = iters
        self.tol = tol
        self.kernel = kernel
        self.X = train_data
        self.y = train_label
        self.m = len(train_data)
        self.C = C
        self.a = np.zeros((1, len(self.X)))
        self.b = 0

    
    def linear_kernel(self, X1, X2):
        ans = X1 @ X2.T
        ans2 = sum([X1[k]*X2[k] for k in range(2)])
        return ans

    def _f(self, i, kernel):
        ans = (self.a * self.y) @ kernel(self.X[i], self.X) + self.b
        return float(ans)

    def _E(self, i, kernel):
        return float(self._f(i, kernel) - self.y[i])
```

### 判断KKT条件&选择合适的$\alpha_1$和$\alpha_2$

```python
def KKT(self, i, kernel):
        y_f = self._f(i, kernel) * self.y[i]
        if self.a[0][i] == 0:
            return y_f >= 1
        elif 0 < self.a[0][i] < self.C:
            return y_f == 1
        else:
            return y_f <= 1

def getal1al2(self, kernel, E):
    index_list = [i for i in range(self.m) if 0 < self.a[0][i] < self.C]
    non_satisfy_list = [i for i in range(self.m) if i not in index_list]
    index_list.extend(non_satisfy_list)
    for i in index_list:
        if self.KKT(i, kernel):
            continue
            j = np.random.randint(0, self.m)
            while j==i: j = np.random.randint(0, self.m)
                return i, j
```

这里取$\alpha_1$和$\alpha_2$用了一种较为简单的方法：选择一个不满足KKT条件的$\alpha_1$，再随机选择一个$\alpha_2$

### 剪辑最优解

```python
def getLH(self, i, j):
        if self.y[i] == self.y[j]:
            L = max(0, self.a[0][i] + self.a[0][j] - self.C)
            H = min(self.C, self.a[0][j] + self.a[0][i])
        else:
            L = max(0, self.a[0][j]-self.a[0][i])
            H = min(self.C, self.C + self.a[0][j] - self.a[0][i])
        return L, H

def clip(self, a, L, H):
    if a > H:
        return H
    elif a < L:
        return L
    else:
        return a
```

### 训练代码

```python
def train(self):
        if self.kernel == 'linear':
            kernel_func = self.linear_kernel
        else:
            raise ValueError('kernel error')
        for num in range(self.iters):
            E = [self._E(i, kernel_func) for i in range(self.m)]
            i, j = self.getal1al2(kernel_func, E)
            E1 = E[i]
            E2 = E[j]
            eta = kernel_func(self.X[i], self.X[i]) + kernel_func(self.X[j], self.X[j]) - 2*kernel_func(self.X[i], self.X[j])
            alpha2_new_unc = self.a[0][j] + self.y[j] * (E1-E2) / eta
            # if eta < 0: continue
            # print("1:", alpha2_new_unc, self.a[0][j])
            L, H = self.getLH(i, j)
            # print("L, H:", L, H)

            alpha2_new = self.clip(alpha2_new_unc, L, H)
            # print("2:", alpha2_new, self.a[0][j])
            alpha1_new = self.a[0][i] + self.y[i] * self.y[j] * (self.a[0][j] - alpha2_new)

            b1_new = -E1 - self.y[i] * kernel_func(self.X[i], self.X[i]) * (alpha1_new - self.a[0][i]) - self.y[j] * kernel_func(self.X[i], self.X[j]) * (alpha2_new - self.a[0][j]) + self.b
            b2_new = -E2 - self.y[i] * kernel_func(self.X[i], self.X[j]) * (alpha1_new - self.a[0][i]) - self.y[j] * kernel_func(self.X[j], self.X[j]) * (alpha2_new - self.a[0][j]) + self.b

            if 0 < alpha1_new < self.C:
                b_new = b1_new
            elif 0 < alpha2_new < self.C:
                b_new = b2_new
            else:
                b_new = (b1_new + b2_new) / 2
            self.b = b_new
            # print(i, self.a[0][i], alpha1_new)
            # print(j, self.a[0][j], alpha2_new)
            # print(self.b, b_new)
            self.a[0][i] = alpha1_new
            self.a[0][j] = alpha2_new
            tol = 0
            y_hat = (self.a * self.y) @ kernel_func(self.X, self.X).T + self.b
            y_pred = np.sign(y_hat)
            y_pred = y_pred.reshape(98)
            acc = sum(y_pred == self.y) / self.m
            print("iter:{}  acc:{}".format(num, acc))
            if acc == 1: break
        w = np.zeros((1, 2))
        for i in range(self.m):
            w += self.a[0][i] * self.y[i] * self.X[i, :]
        return w, self.b
```

### 结果

![image-20220605161957098](https://te4p0t.github.io/assets/images/typora-user-images/202206141104770.png)

![image-20220605162007391](https://te4p0t.github.io/assets/images/typora-user-images/202206141105197.png)

## 完整代码

```python
import numpy as np
import matplotlib.pyplot as plt

data = np.loadtxt('./data.txt')
# print(data)
train_data = data[:,:2]
train_label = data[:,-1]
train_label = np.where(train_label != 0, train_label, -1)
print(len(train_data))


class SVM(object):
    def __init__(self, iters, tol, kernel, C, train_data, train_label):
        self.iters = iters
        self.tol = tol
        self.kernel = kernel
        self.X = train_data
        self.y = train_label
        self.m = len(train_data)
        self.C = C
        self.a = np.zeros((1, len(self.X)))
        self.b = 0

    
    def linear_kernel(self, X1, X2):
        ans = X1 @ X2.T
        ans2 = sum([X1[k]*X2[k] for k in range(2)])
        return ans

    def _f(self, i, kernel):
        ans = (self.a * self.y) @ kernel(self.X[i], self.X) + self.b
        return float(ans)

    def _E(self, i, kernel):
        return float(self._f(i, kernel) - self.y[i])

    def KKT(self, i, kernel):
        y_f = self._f(i, kernel) * self.y[i]
        if self.a[0][i] == 0:
            return y_f >= 1
        elif 0 < self.a[0][i] < self.C:
            return y_f == 1
        else:
            return y_f <= 1

    def getal1al2(self, kernel, E):
        index_list = [i for i in range(self.m) if 0 < self.a[0][i] < self.C]
        non_satisfy_list = [i for i in range(self.m) if i not in index_list]
        index_list.extend(non_satisfy_list)
        for i in index_list:
            if self.KKT(i, kernel):
                continue
            j = np.random.randint(0, self.m)
            while j==i: j = np.random.randint(0, self.m)
            return i, j

    def getLH(self, i, j):
        if self.y[i] == self.y[j]:
            L = max(0, self.a[0][i] + self.a[0][j] - self.C)
            H = min(self.C, self.a[0][j] + self.a[0][i])
        else:
            L = max(0, self.a[0][j]-self.a[0][i])
            H = min(self.C, self.C + self.a[0][j] - self.a[0][i])
        return L, H

    def clip(self, a, L, H):
        if a > H:
            return H
        elif a < L:
            return L
        else:
            return a

    def train(self):
        if self.kernel == 'linear':
            kernel_func = self.linear_kernel
        else:
            raise ValueError('kernel error')
        for num in range(self.iters):
            E = [self._E(i, kernel_func) for i in range(self.m)]
            i, j = self.getal1al2(kernel_func, E)
            E1 = E[i]
            E2 = E[j]
            eta = kernel_func(self.X[i], self.X[i]) + kernel_func(self.X[j], self.X[j]) - 2*kernel_func(self.X[i], self.X[j])
            alpha2_new_unc = self.a[0][j] + self.y[j] * (E1-E2) / eta
            # if eta < 0: continue
            # print("1:", alpha2_new_unc, self.a[0][j])
            L, H = self.getLH(i, j)
            # print("L, H:", L, H)

            alpha2_new = self.clip(alpha2_new_unc, L, H)
            # print("2:", alpha2_new, self.a[0][j])
            alpha1_new = self.a[0][i] + self.y[i] * self.y[j] * (self.a[0][j] - alpha2_new)

            b1_new = -E1 - self.y[i] * kernel_func(self.X[i], self.X[i]) * (alpha1_new - self.a[0][i]) - self.y[j] * kernel_func(self.X[i], self.X[j]) * (alpha2_new - self.a[0][j]) + self.b
            b2_new = -E2 - self.y[i] * kernel_func(self.X[i], self.X[j]) * (alpha1_new - self.a[0][i]) - self.y[j] * kernel_func(self.X[j], self.X[j]) * (alpha2_new - self.a[0][j]) + self.b

            if 0 < alpha1_new < self.C:
                b_new = b1_new
            elif 0 < alpha2_new < self.C:
                b_new = b2_new
            else:
                b_new = (b1_new + b2_new) / 2
            self.b = b_new
            # print(i, self.a[0][i], alpha1_new)
            # print(j, self.a[0][j], alpha2_new)
            # print(self.b, b_new)
            self.a[0][i] = alpha1_new
            self.a[0][j] = alpha2_new
            tol = 0
            y_hat = (self.a * self.y) @ kernel_func(self.X, self.X).T + self.b
            y_pred = np.sign(y_hat)
            y_pred = y_pred.reshape(98)
            acc = sum(y_pred == self.y) / self.m
            print("iter:{}  acc:{}".format(num, acc))
            if acc == 1: break
        w = np.zeros((1, 2))
        for i in range(self.m):
            w += self.a[0][i] * self.y[i] * self.X[i, :]
        return w, self.b

svm = SVM(100, 0.00001, "linear", 10, train_data, train_label)
w, b= svm.train()
print(w)

for i in range(len(train_data)):
    x1 = train_data[i][0]
    x2 = train_data[i][1]
    if train_label[i] == 1:
        plt.scatter(x1, x2, c='r')
    else:
        plt.scatter(x1, x2, c='b')
x = np.arange(-3, 5, 0.1)
y=(-b-w[0][0]*x)/w[0][1]
x.shape=(len(x),1);y.shape=(len(x),1) 
plt.plot(x,y,linewidth=2.0,label="Boarder")
plt.show()
```

