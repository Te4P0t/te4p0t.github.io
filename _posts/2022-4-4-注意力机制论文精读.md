---
layout: post
title: 注意力机制论文精读
subheading: 读读论文
author: te4p0t
categories: ReadPaper
tags: Machine-Learning Computer-Vision Read-Paper
---

# 注意力机制论文精读

## FcaNet（ICCV21）

**Frequency Channel Attention Networks**

### 想法

通道注意力，通过某种方式得到一个标量，来表示通道维度上的特征信息。将通道注意力的问题，看做是图像信息频域分析的压缩。希望找到一种方式，能够将一个通道的信息紧密的压缩成一个标量，同时尽可能保存整个通道的特征信息。

使用DCT(discrete cosine transform)

理由

1. DCT是信号处理中常用的压缩信息的手段，尤其是在图片和视频上。DCT在保证高质量的同时又很强的压缩率。符合要求。
2. DCT可以通过element-wise的乘法实现，并且是可微的，因此能够插入CNN中
3. DCT可以看做是GAP(global average pooling)的一种泛化模式。从数学上，GAP等效于DCT在最低频率上做的压缩部分。

本文设计了一种multi-spectral channel attention(MSCA) framework。



![image-20220401235729307](https://te4p0t.github.io/assets/images/typora-user-images/202204272234379.png)

![image-20220401235847357](https://te4p0t.github.io/assets/images/typora-user-images/202204272234125.png)

在通道注意力中，GAP值保留了最低频率的信息，抛弃了其他频率。

### Multi-Spectral Channel Attention Module

![image-20220402000059973](https://te4p0t.github.io/assets/images/typora-user-images/202204272234046.png)

### 三种选取频度方法

1. LF（low-frequency) 因为一些方法显示CNN更适应低频信息，因此只选取低频。
2. TS (two-step) 先单独对每种u, v（频度）取值做评估，然后选取Top-k作为最终取值。
3. NAS(neural architecture search) 用神经网络架构搜索

![image-20220402110911524](https://te4p0t.github.io/assets/images/typora-user-images/202204272234961.png)



### 消融实验

**频度选择**

![image-20220402111338314](https://te4p0t.github.io/assets/images/typora-user-images/202204272234353.png)

1. 验证了深度神经网络偏好于低频信息（SENet的成功）
2. 除了最高频的，其他和最低频结果之间差距都小于0.5％

**频度数量（K）的影响**

![image-20220402111754169](https://te4p0t.github.io/assets/images/typora-user-images/202204272234916.png)

![image-20220402112026632](https://te4p0t.github.io/assets/images/typora-user-images/202204272234483.png)

deep networks are redundant. If two channels are redundant for each other, we can only get the same information using GAP. However, in our multi-spectral framework, it is possible to extract more information from redundant channels because different frequency components contain different information.

### 结果对比

**ImageNet**

![image-20220402112611257](https://te4p0t.github.io/assets/images/typora-user-images/202204272234092.png)

**COCO物体分割**

![image-20220402112909805](https://te4p0t.github.io/assets/images/typora-user-images/202204272234044.png)

**COCO语义分割**

![image-20220402113002634](https://te4p0t.github.io/assets/images/typora-user-images/202204272234081.png)

### 本文贡献

1. 将通道注意力视作压缩任务，引入了DCT方法。并证明了GAP是DCT的一种特殊情况。提出了一种multi-spectral通道注意力结构，即FcaNet。
2. 提出了三种选择频率组成的方式。
3. 通过实验证明了这种方法在ImageNet和COCO上state-of-the-art，同时和SENet的计算开销相同。



## SGENet（CoRR19）

**Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks**

![image-20220402123303328](https://te4p0t.github.io/assets/images/typora-user-images/202204272234759.png)

### 想法

分组的思想在计算机视觉中由来已久，早期视觉任务中人工设计特征SIFT, HOG等，再到CNN中分组卷积，分组正则。通过在通道方向上分组，学习不同自特征来表示不同语义。

结果表明SGE显著改善了不同语义子特征在其group内的空间分布，并产生了较大的统计方差，增强了语义区域的特征学习，压缩了噪声和干扰

### 方法

受到capsules思想的启发，假设每组特征图都得到了特定的语义（如狗的眼睛）。在这个group space中，我们可以得到在眼睛这个位置有强响应的特征（ features with a larger vector length and similar vector directions among multiple eye regions），而其他位置未被激活为0向量。

由于噪声和相似区域的影响，CNN很难得到良好分布的特征响应。为了解决这个问题，使用整个group space的信息来加强该区域内语义特征的学习，因为全局信息不会被噪声支配。

![image-20220402131653748](https://te4p0t.github.io/assets/images/typora-user-images/202204272234488.png)

![image-20220402131700059](https://te4p0t.github.io/assets/images/typora-user-images/202204272235626.png)

（其实就是在组内做了一个gap，然后点乘在组特征图上。）

点乘可以写作![image-20220402131743679](https://te4p0t.github.io/assets/images/typora-user-images/202204272239928.png)形式，theta表示向量之间角度。因此长度较长并且方向类似的特征保留下来（应该意思是类似于抑制噪声的操作)

![image-20220402134126694](https://te4p0t.github.io/assets/images/typora-user-images/202204272235172.png)

正则化+sigmoid后再element-wise乘在原图上





### 可视化和可解释性

![image-20220402140843013](https://te4p0t.github.io/assets/images/typora-user-images/202204272235941.png)

将Group设置为64做了实验，**经验性**地发现第18组，22组和41组分别对应鼻子，舌头和眼睛。SGE有效的增强了特征表示，抑制了噪声。第4行和第7行即使眼睛闭着仍被SGE强调出来了，而ResNet则不行。

**统计角度**

理想的特征图网络空间激活值有更明显地对比，即在语义相关区域数值较大，非相关区域则基本无表示。在统计上的表现就是有更大的方差或稀疏性。

![image-20220402135413487](https://te4p0t.github.io/assets/images/typora-user-images/202204272235979.png)

实验结果和猜想一致

![image-20220402142036134](https://te4p0t.github.io/assets/images/typora-user-images/202204272235092.png)

第一组的统计直方图，较小的数值趋近于0，较大的数值则基本维持不变。从统计学角度说明了抑制噪声并增强关键区域。

### 消融实验

**组数**

![image-20220402142311602](https://te4p0t.github.io/assets/images/typora-user-images/202204272235430.png)

较多分组减小了每组内自特征维度，降低了语义特征表示能力。而较小的分组则限制了语义多样性。存在一个超参数G达到平衡。

**初始化$\gamma$和$\beta$**

![image-20220402142322369](https://te4p0t.github.io/assets/images/typora-user-images/202204272235534.png)

可以看到$\gamma$初始为0时效果更好。推断在神经网络学习的前期，由于卷积特征图的语义学习还没有初步完成，因此可以暂时放弃注意力的学习，让网络先训练语义特征。在之后逐渐将注意力机制有效化。

**有无归一化**

![image-20220402142325821](https://te4p0t.github.io/assets/images/typora-user-images/202204272235546.png)

由于同一语义组中不同样本生成的特征分布不一致，不进行归一化很难获得鲁棒的重要系数。

这说明同一组不同样本的激活值的方差在统计上可能有很大的差异，说明归一化是SGE工作的必要条件

### 结果对比

**ImageNet**

![image-20220402142156889](https://te4p0t.github.io/assets/images/typora-user-images/202204272235669.png)

**COCO**

![image-20220402143700659](https://te4p0t.github.io/assets/images/typora-user-images/202204272235898.png)

![image-20220402143718723](https://te4p0t.github.io/assets/images/typora-user-images/202204272235864.png)

### 本文贡献

1. 提出了Spatial Group-wise Enhance(SGE)模块，能够增强特征图中的语义表示同时抑制噪声。几乎不引进其他参数和计算复杂度。
2. 可视化角度，证实了每个feature group具有表示不同语义的能力，同时SGE模块加强了这个能力。



## SA-NET（ICASSP 21）

SA-NET: SHUFFLE ATTENTION FOR DEEP CONVOLUTIONAL NEURAL NETWORKS

### 想法

SGE引入分组策略加快了计算速度，但并没有充分利用空间和通道注意力的关联，使它less efficient。

同时参考ShuffleNet v2中，通过“channel shuffle operator”来加强两个分支中的通信。

基于以上观察，本文提出了一种更轻量但有效的Shuffle Attention(SA)模块。它也通过通道分组来学习子特征。对每个子特征，SA模块采用Shuffle单元来同时构建通道和空间注意力。同样设计了一个注意力掩码来抑制可能的噪声同时强调正确的语义特征区域（和SGE一样）。

### Shuffle Attention

![image-20220402203250031](https://te4p0t.github.io/assets/images/typora-user-images/202204272235529.png)

先将输入分为G个组$X=[X1, ...,X_G], X_K\in \mathbb{R^{C/G \times H \times W}} $，每组去学习特定的语义响应。

之后，在每个注意力单元开始，将$X_K$再分为两个分支，$X_{k1},X_{k2} \in \mathbb{R^{C/2G \times H \times W}}$。一个分支用来生成通道注意力图，来确定通道间的内在关系，另一个分支用来生成空间注意力图来利用特征空间关系。

**通道注意力**

用SE block会带来很多参数，同时ECA使用k大小的一维卷积也不合适，因为k往往很大。

本文首先采用GAP $\displaystyle s = \mathcal{F_{gp}}(X_{k1})=\frac{1}{H\times W}\sum \limits^{H}_{i=1}\sum\limits^{W}_{j=1}{X_{k1}(i, j)}$

之后使用通过一个简单的门控机制和sigmoid函数来获得一个压缩的特征

$X_{k1}'=\sigma(\mathcal{F_c(s)})\cdot X_{k1} = \sigma(W_1s+b_1)\cdot X_{k1}$

其中$W_1 , b_1\in \mathbb{R^{C/2G \times 1 \times 1}} $

**空间注意力**

首先通过 Group Norm得到空间上的统计信息，之后再同样一个$\mathcal{F_c(\cdot)}$作用在上面

$X_{k2}'=\sigma(W_2\cdot GN(X_{k2})+b_2)\cdot X_{k2}$

之后将得到的两个特征图$[X_{k1}', X_{k2}']$在通道维度上拼接，得到和输入相同尺寸的特征图

**聚合**

将所有组拼接后，与ShuffleNet v2相似，使用一个“channel shuffle”操作来使跨组别的信息在通道维度传递。

在所有过程中$W_1, b_1, W_2, b_2$和Group Norm的超参数是仅有的额外引入的参数，每个分支的通道数是$C/2G$，所以总参数量是$3C/G$，相较于整个网络百万级别的参数微不足道。

**实现**

![image-20220403000455676](https://te4p0t.github.io/assets/images/typora-user-images/202204272235840.png)

### 可视化和可解释性

**有无channel shuffle**

![image-20220403001000358](https://te4p0t.github.io/assets/images/typora-user-images/202204272235370.png)

可见加了shuffle操作后，显著提高了分类得分

**网络不同阶段**

统计了网络不同阶段平均激活值。

![image-20220403001425766](https://te4p0t.github.io/assets/images/typora-user-images/202204272235655.png)

发现：

1. 在网络的前期，各类别在各个组上的分布都是非常相似的，which suggests that the importance of feature groups is likely to be shared by different classes in early stages。
2. 在更深的层数，各类别的平均激活值变得差异了起来。
3. SA_5_2对不同类仍是相似的，说明SA_5_2相较于其他block是不那么重要的

**可视化**

![image-20220403001933797](https://te4p0t.github.io/assets/images/typora-user-images/202204272235622.png)

### 本文贡献

1. 设计了一个轻量并且有效的注意力模块，SA module。
2. 大量实验证明SANet有更低的模型复杂度但效果优于SOTA attention。

### 结果对比

**ImageNet**

和其他方法对比

![image-20220403002016246](https://te4p0t.github.io/assets/images/typora-user-images/202204272235022.png)

自身消融实验

![image-20220403002041490](https://te4p0t.github.io/assets/images/typora-user-images/202204272236894.png)

**COCO物体检测**

![image-20220403002149659](https://te4p0t.github.io/assets/images/typora-user-images/202204272236916.png)

**COCO实例分割**

![image-20220403002208508](https://te4p0t.github.io/assets/images/typora-user-images/202204272236703.png)

## Triplet Attention

Rotate to Attend: Convolutional Triplet Attention Module

### 想法

来源于CBAM，但是认为CBAM中求通道注意力时进行数据降维（减小通道数）对捕捉通道间的非线性局部依赖是冗余的，同时没有得到作者强调的跨维度的（cross-dimension）交互

![image-20220403004108952](https://te4p0t.github.io/assets/images/typora-user-images/202204272236584.png)

### 方法

传统方法通过GAP将一个通道压缩为一个值，这个过程大量损失了空间信息。CBAM加入空间注意力，但通道注意力和空间注意力的计算是独立的。因此，两者间的联系未被考虑到。

Triplet Attention将输入分为三个分支，两个分支用来获得C和W或C和H的跨通道信息，剩下一个和CBAM一样建立空间注意力。每个分支通过简单的重排列，进行Z-pool，之后k\*k卷积，sigmoid激活。

和之前的通道注意力相比，该方法有两个有点

1. 以微不足道的计算开销得到了丰富的特征表示。
2. 和前面的方法不同，该方法强调在不降维的情况下进行跨维度的交互。

![image-20220403115427342](https://te4p0t.github.io/assets/images/typora-user-images/202204272236296.png)

### 方法细节

三个分支获得的结果通过简单的取均值聚合。

**Z-pool**

$Z-pool(\mathcal{X})=[\rm{MaxPool}_{0d}(\mathcal{X}), \rm{AvgPool_{0d}(\mathcal{X})}]$

Z-pool层通过将average pooled和max pooled两张图拼接，将输入在第0个维度上降低到2。这使得该层能够保留真实tensor的丰富表征信息，压缩深度便于计算。

**Three branch**

第一个分支用来得到高H和通道C的联系。

通过将输入按H所在的轴逆时针旋转90°实现。之后进行Z-pool变为$（2\times H \times C）$，得到的结果输入进行k\*k卷积，得到$(1\times H \times C)$，之后用sigmoid激活，之后乘回去，在按H轴顺时针旋转90°得到输出，维持和输入同向。

之后两个分支与第一个分支类似。

最终的输出$\displaystyle y = \frac{1}{3}(\overline{(\mathcal{X_1}\sigma(\psi_1(\mathcal{X_1^*}))} + \overline{(\mathcal{X_2}\sigma(\psi_1(\mathcal{X_2^*}))} + \mathcal{X}\sigma(\psi(\mathcal{X_3}))$

$\psi_i$代表$i$分支中的标准k\*k的二维卷积。

简化后![image-20220403143632878](https://te4p0t.github.io/assets/images/typora-user-images/202204272236785.png)

**复杂性分析**

![image-20220403143802489](https://te4p0t.github.io/assets/images/typora-user-images/202204272236562.png)

$C$表述输入通道数，$r$表示通道缩放率，$k$表示卷积核大小。而k是远小于C的。所以Triplet Attention参数量很小

### 消融实验

![image-20220403145824470](https://te4p0t.github.io/assets/images/typora-user-images/202204272236130.png)

spatial off表示第三个分支关掉。channel off表示前两个分支关掉。

full表示完整的Triplet Attention。

结果表明完整的效果总是好于前两者。

**可视化**

![image-20220403150059103](https://te4p0t.github.io/assets/images/typora-user-images/202204272236780.png)



### 结果对比

**ImageNet**

![image-20220403145242043](https://te4p0t.github.io/assets/images/typora-user-images/202204272236101.png)

**COCO物体检测**

![image-20220403145456031](https://te4p0t.github.io/assets/images/typora-user-images/202204272236614.png)

**PASCAL VOC物体检测**

![image-20220403145523297](https://te4p0t.github.io/assets/images/typora-user-images/202204272236912.png)



## AFF

Attentional Feature Fusion

### 想法

特征融合在现在的网络结构中很常见，通常用简单的线性操作，如求和，拼接等实现，但是这些方法可能不是最好的选择。本文提出了一种规范和统一的方式，称作Attentional feature fusion。

为了更好地融合语义和尺度不一致的特征，我们提出了一种multi scale channel attention module。

SKNet和ResNeSt注意力特征融合存在的问题：

1. 场景限制：SKNet和ResNeSt只关注同一层的特征选择，无法做到跨层特征融合。
2. 简单的初始继承：为了将得到的特征提供给注意力模块，SKNet通过相加来进行特征融合，而这些特征在规模和语义上可能存在很大的不一致性，对融合权值的质量也有很大影响。使模型表现受限。
3. 偏向上下文的尺度聚合：SKNet和ResNeSt的融合权值时通过全局通道注意力机制生成的，对于分布更全局的特征，这种机制更有效，但是对于小目标效果就不好。能否让网络自适应地动态地学习不同尺度特征。

### Multi-scale Channel Attention

使用点卷积（1\*1卷积）作为通道上下文聚合器

![image-20220403231106168](https://te4p0t.github.io/assets/images/typora-user-images/202204272236696.png)

其中$\rm{PWConv_1}$和$\rm{PWConv_2}$的卷积核大小分别为$\displaystyle \frac{C}{r} \times C \times 1 \times 1$和$\displaystyle C \times \frac{C}{r} \times 1 \times 1$

最终的$\rm{L(X)}$和输入尺寸相同，可以保存并突出低层特征的细节。

最终的特征图为![image-20220403231721934](https://te4p0t.github.io/assets/images/typora-user-images/202204272236998.png)

其中$\rm{g(x)}$就是SE block中的操作。

![image-20220403231833755](https://te4p0t.github.io/assets/images/typora-user-images/202204272236429.png)

### Attentional Feature Fusion

两张特征图$\rm{X, Y} \in \mathbb{R}^{C \times H\times W}$

假设$\rm{Y}$是更大感受野的特征图，具体而言

1. 同层的情况，在InceptionNet中$\rm X$是3\*3卷积的输出，$\rm Y$是5\*5卷积的输出。
2. 短连接（short skip connection）的情况，$\rm X$是identity mapping，$\rm Y$是学习的残差
3. 长链接（long skip connection）的情况，$\rm X$是低层特征图，$\rm Y$是高层特征图。

基于Mutil-scale channel attention模块进行特征融合

![image-20220403233620412](https://te4p0t.github.io/assets/images/typora-user-images/202204272237820.png)

其中$\rm{Z} \in \R^{C\times H \times W}$是融合后的特征图，$\rm M$是Mutil-scale channel attention模块，$\uplus$是初步特征融合。

![image-20220404160027013](https://te4p0t.github.io/assets/images/typora-user-images/202204272237831.png)

假设特征初步融合的方式是加，则AFF结构如上图。

![image-20220404160319646](https://te4p0t.github.io/assets/images/typora-user-images/202204272237208.png)

不同网络的特征融合方式

尽管不同方式的实现之间有差别，但用数学公式表示出来，各种方式的细节上的差别就没有了，因此可以用某种精心设计的特征融合方式去取代原先的方式，从而来提升性能。

![image-20220404161542035](https://te4p0t.github.io/assets/images/typora-user-images/202204272237902.png)

以Inception, ResBlock, FPN为例

### Iterative Attentional Feature Fusion

![image-20220404161421028](https://te4p0t.github.io/assets/images/typora-user-images/202204272237191.png)

### 结果对比

![image-20220404163532438](https://te4p0t.github.io/assets/images/typora-user-images/202204272237702.png)

![image-20220404163554525](https://te4p0t.github.io/assets/images/typora-user-images/202204272237919.png)

![image-20220404163614327](https://te4p0t.github.io/assets/images/typora-user-images/202204272237795.png)

![image-20220404164827408](https://te4p0t.github.io/assets/images/typora-user-images/202204272237153.png)

### 可视化

![image-20220404162920290](https://te4p0t.github.io/assets/images/typora-user-images/202204272237557.png)

![image-20220404162936193](https://te4p0t.github.io/assets/images/typora-user-images/202204272237959.png)

SENet-50主义的区域是过大的，包括了很多背景。是因为SE值利用了全局通道注意力，更偏向于全局尺度的上下文信息。MS-CAM同时聚合了局部通道信息，帮助网络减小了对背景的关注，同时有益于小物体的识别。

## GSA（ICLR21）

GLOBAL SELF-ATTENTION NETWORKS FOR IMAGE RECOGNITION

### 想法

现有的自注意力模型，因为计算和空间的复杂性，要么只在网络后期的低分辨特征图上做，要么只把感受野限制在每层的一个小区域。

为了克服这种限制，本文提出了一种新的global self-attention module，称作GSA。

该模块由两个平行的层，一个content attention layer，一个positional attention layer。输出是两层的和。

### 比较

作为网络辅助的注意力机制，他们将绝大部分特征提取的工作交给了卷积操作。而GSA使用注意力作为基本的操作，取代了卷积。

### Method

![image-20220404125708613](https://te4p0t.github.io/assets/images/typora-user-images/202204272238439.png)

![image-20220405000047919](https://te4p0t.github.io/assets/images/typora-user-images/202204272238054.png)分别表示GSA module中的输入的特征图和输出的特征图。

![image-20220405000145782](https://te4p0t.github.io/assets/images/typora-user-images/202204272238935.png)

均为通过1\*1卷积得到的

**Content Attention Layer**

![image-20220405000238445](https://te4p0t.github.io/assets/images/typora-user-images/202204272238165.png)



## CoordAttention（CVPR21）

Coordinate Attention for Efficient Mobile Network Design时

### 想法

很多注意力模型因为计算开销太大所以不适用于mobile networks。在有限的开销内，最主流的方法仍然是SE block，但是SE只编码了通道间的信息，却忽略了位置信息的重要性，这对视觉任务获取物体结构很重要。之后的工作，如BAM和CBAM，尝试压缩通道来得到位置信息，从而用卷积计算空间注意力，但是卷积只能获得局部依赖，不能建模长距离依赖。

本文提出了一种将位置信息嵌入通道注意力中，从而避免过大开销的方法。为了避免SENet那样二维池化使位置信息丢失，我们用两个平行的一维特征编码，来有效地将空间轴的信息融入生成的注意力图上。

**优点**

仅仅通过跨通道的操作但能同时注意到方向和位置敏感的信息。同时是轻量灵活的。

### Coordinate Attention

![image-20220405141651465](https://te4p0t.github.io/assets/images/typora-user-images/202204272238178.png)

**Coordinate Information Embedding**

对于一个给定的输入$\rm X$，通过两个大小分别为$(H, 1) (1, W)$的pooling层去在水平和垂直两个维度上编码一个通道。用公式可以分别写作

![image-20220405142009991](https://te4p0t.github.io/assets/images/typora-user-images/202204272238016.png)

![image-20220405142016605](https://te4p0t.github.io/assets/images/typora-user-images/202204272238762.png)

这两个变换也使得注意力模块可以在空间方向上捕捉长距离依赖并且保存位置信息。帮助网络更精确的定位物体。

**Coordinate Attention Generation**

设计原则

1. 这个变换要简单且开销少
2. 要充分利用捕捉到的位置信息
3. 要能够有效捕捉通道间关系。

首先将之前得到的两个特征图在通道维度上concate，然后用一个1*1卷积压缩通道，$r$为压缩率，之后sigmoid激活（和SE一样的操作），得到了一张$\rm{f}\in \R{^{C/r \times (H+W)}}$的特征图

之后在空间维度上将得到的结果再进行拆分$\rm{f}^h \in \R^{C/r \times H}$和$\rm{f}^w \in \R^{C/r \times W}$

之后在用两个1*1卷积分别作用域两张图，用sigmoid函数激活后，得到$\rm g^h$和$\rm g^w$

最终的输出$y_c(i,j)=x_c(i,j)\times g_c^h(i) \times g_c^w(j)$

为了减小参数量，通常选择一个合适的压缩率$r$（例如32）

![image-20220406103747703](https://te4p0t.github.io/assets/images/typora-user-images/202204272238101.png)

### 消融实验

![image-20220406104512907](https://te4p0t.github.io/assets/images/typora-user-images/202204272238261.png)

![image-20220406104852425](https://te4p0t.github.io/assets/images/typora-user-images/202204272238311.png)

![image-20220406104919493](https://te4p0t.github.io/assets/images/typora-user-images/202204272238038.png)

**$r$的取值**

![image-20220406105048570](https://te4p0t.github.io/assets/images/typora-user-images/202204272239465.png)

### 对比实验

![image-20220406105245344](https://te4p0t.github.io/assets/images/typora-user-images/202204272238984.png)

该方法相对于CBAM在位置信息上的编码有两个方面的优势

1. 位置信息在CBAM中被压缩到一个通道，导致了信息丢失。而该方法使用了合适的压缩率去压缩通道，避免了太多的信息丢失。（但同时增大了开销）
2. CBAM使用了7*7卷积来编码局部空间信息。而该方法用1维全局池化操作，使得该注意力能够捕捉长距离依赖信息。

![image-20220406105716048](https://te4p0t.github.io/assets/images/typora-user-images/202204272239964.png)

**COCO**

![image-20220406105804870](https://te4p0t.github.io/assets/images/typora-user-images/202204272239089.png)

**Pascal VOC 2007**

![image-20220406105825516](https://te4p0t.github.io/assets/images/typora-user-images/202204272239785.png)

**cityspaces**

![image-20220406105843371](https://te4p0t.github.io/assets/images/typora-user-images/202204272239830.png)

## $BA^2M$

BA2M: A Batch Aware Attention Module for Image Classification
