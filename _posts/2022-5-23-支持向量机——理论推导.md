---
layout: post
title: 支持向量机——理论推导
subheading: SMO
author: te4p0t
categories: Machine-Learning
tags: Machine-Learning Computer-Vision
---

# 支持向量机——理论推导

## 一、引言

​	支持向量机（Support Vector Machines)是在统计学习理论的基础上发展起来的学习算法，由弗拉基米尔·瓦普尼克（Vladimir N. Vapnik）及其合作者发明，在1992年计算学习理论的会议上介绍进入机器学习领域，之后受到了广泛关注。该算法在文本分类、手写识别、图像分类、生物信息学等领域中获得了较好的应用。

​	支持向量分类的目的是开发计算有效的途径，从而能在高维特征空间中学习泛化能力好的分类超平面。

## 二、具体算法

### 1.线性模型

对于一个问题，假设给定的训练样本集在样本空间中的分布如下图所示（图来自周志华的《机器学习》），能将该训练样本分开的“超平面”有无数个，哪一个是最“好”的划分超平面呢？

<img src="https://te4p0t.github.io/assets/images/typora-user-images/202206141101446.jpg" alt="img" style="zoom: 25%;" />

比较直观的感觉是选取“最中间”的红线，从样本集的角度来说，一方面样本在各特征上的取值是由我们测量所得，存在一定误差。另一方面，可能存在我们没有取到的样本，在空间中的分布更加扩散。那么，“最中间”的这条红线能使模型的容错率和泛化性能最佳。

考虑一个二分类问题，定义：对于任意一个可以将训练样本集划分的超平面，对每类样本，将超平面在样本空间中向该类样本所在的方向平移，过程中所交的第一个该类样本称为**支持向量**，超平面在两类样本的支持向量间平移的距离称为**间隔**。对于该问题最好的划分平面为**使间隔达到最大，且到两类支持向量距离相等**的超平面，称为**最大间隔超平面**。

<img src="https://te4p0t.github.io/assets/images/typora-user-images/202206141100067.png" alt="image-20220521173704447" style="zoom: 80%;" />

**定义：**

1. 训练数据及标签${\{(x_i, y_i)\}_{i=1\sim N}}$,$$(X_1, y_1),(X_2, y_2),...,(X_n, y_n)$$，其中$$X_i$$为第$i$个样本的取值向量，向量长度取决于特征维数。$$y_i$$为第$i$个样本的标签值，其值为$+1$或$-1$。

2. 线性模型$$(W, b)$$，超平面为$$W ^TX+b=0$$，其中$W$的维度与$X$维度相同，$b$是一个常数。

3. 一个训练集线性可分是指

   $\exists(W, b)$，使：$ \forall {i=1\sim N}$有：

   1. 若$y_i=+1$，则$W^TX_i+b\ge0$
   1. 若$y_i=-1$，则$W^TX_i+b\lt0$

因为标签的取值只能为$+1$或$-1$，所以定义3可以结合为一个等价的式子
$$
y_i[W^TX_i+b] \ge 0 \tag 1
$$
之后我们需要知道以下两个**事实**

1. $W^TX+b=0$与$aW^TX+ab=0$是同一个平面，其中$a\in \mathbb{R^+}$。（也可以理解为若$(W, b)$满足公式1，则$(aW, ab)$也满足公式1。）
2. 二维情况下，点$(x_0, y_0)$到平面$w_1x+w_2y+b=0$的距离公式：$d = \displaystyle \frac{|w_1x_0 + w_2y_0+b|}{\sqrt{w_1^2+w_2^2}}$。推广到高维，向量$X_0$到超平面$W^TX+b=0$的距离$d = \displaystyle \frac{|W^TX_0+b|}{||W||}$

基于事实1，我们可以用$a$去缩放$(W, b) \rightarrow (aW, ab)$，使在支持向量$X_0$上有：$|W^TX_0+b|=1$。此时由事实2，支持向量与平面的距离$\displaystyle d=\frac{1}{||W||}$，所以我们最大化间隔$d$也就等价于最小化$||W^2||$。

又因为支持向量是每类样本中距划分超平面距离最近的点，所以对于所有样本点都有$(W^TX_i+b)\le -1$或$(W^TX_i+b)\ge1$，此时将类别标签引入可以统一符号，得到支持向量机的限制条件$y_i(W^TX_i+b) \ge 1$。

因此最终可得支持向量机**优化问题**：
$$
\min\limits_{W,b}||W||^2 \\ s.t. \quad y_i(W^TX_i+b)\ge 1\quad(i = 1\sim N) \tag 2
$$
对于一个线性可分的样本集，上述优化问题一定能够确定一个唯一的划分超平面。而若是非线性可分的问题，因为不能完全满足限制条件，所以无解。在这里我们先不介绍如何去求解这个优化问题，我们在后面非线性模型的时候再去介绍。

### 2.二次规划

上述支持向量机优化问题实际上是凸优化问题中**二次规划（Quadratic Programming）**的形式

二次规划满足以下条件

1. 目标函数（Objective Function）为二次项。
2. 限制条件为一次项。

对于一个满足上述条件的二次规划问题，要么无解，要么只有一个极值。（局部极值就是全局极值）

### 3.非线性模型——优化目标改造

线性模型要点：

1. SVM是最大化间隔的分类算法。

2. 训练样本${(X_i, y_i)}_{i=1\sim N}$

3. 优化目标

   最小化：$\displaystyle \frac{1}{2}||W||^2$

   限制条件：$y_i(W^TX_i+b)\ge1 (i=1\sim N)$

SVM处理非线性问题：

首先要对线性模型进行改造，使得线性模型中的优化目标能够处理非线性情况。这个问题其实不难，只需要对优化目标进行一点点改造
$$
\min\limits_{W, b} \frac{1}{2}||W||^2+C\sum \limits_{i=1}^N\xi_i \\ s.t. \quad y_i(W^TX_i+b)\ge 1 \quad (i=1\sim N) \\ \xi_i \ge 0 \tag 3
$$
其中$\xi_i$为松弛变量（Slack Variable），因为线性模型中的限制条件，在非线性问题中无法被完全满足。引入了$\xi_i$后就能够被满足。极端情况下，若$\xi_i\ge1$，那么该限制条件必然满足。我们将这种方法称为**软间隔(soft margin)**。但是我们在优化问题时，又不能让$\xi_i$过于大，否则该优化问题会很发散，因此在最小化目标中加入了一项$C\sum\limits^N_{i=1}\xi_i$，我们通常将这一项称为**正则项(Regulation Term)**，其中$C$是一个预先设置的参数，可以理解为控制正则项的强度，用来平衡最小化目标中两项的权重。因此，最小化的目标承担着两个任务，一是最大化间隔，二是使$\xi_i$尽可能小。

### 4.非线性模型——低维到高维的映射

上述对优化目标的改造使得模型在遇到非线性问题时也能找到一个满足限制条件的解。但是上述条件只是允许了某些样本不满足之前的约束，但对于某些线性不可分的问题，并不能从实质上改善模型性能。如下图所示的例子中，引入的**软间隔**和**正则项**的方法，使得模型从无解，到能够训练得出如下的解，但实际上对分类的准确性并没有帮助。

<img src="https://te4p0t.github.io/assets/images/typora-user-images/202206141101988.png" alt="image-20220524201904816" style="zoom:67%;" />

通过观察，我们可以发现，对于这样一个问题，我们需要拟合出来一个椭圆形状的划分平面，但支持向量机的性质决定了它只会去寻找线性超平面，我们如何扩展SVM的性能使之能解决这些问题呢？

#### 高维映射

基于这样一个事实：**在低维空间内线性不可分的数据集，映射到高维空间中，将会有更大的概率能够被线性分开。**

定义：**高维映射**$\phi(x)$，使得$x \rightarrow \phi{(x)}$，其中$x$是一个低维向量，而$\phi{(x)}$是一个高维向量。

**一个简单的例子：**

异或问题有四个样本$X_1 = [0, 0] \in C_1,\quad X2=[1, 1]\in C_1,\quad X_3 = [0, 1]\in C_2,\quad X_4=[1, 0]\in C_2$，我们定义$\phi{(x)}:X=[a,b] \rightarrow \phi{(x)}=[a^2, b^2, a, b, ab]$，对四个样本分别进行映射，得到结果$\phi{(X_1)}=[0,0,0,0,0],\:\phi{(X_2)=[1,1,1,1,1]},\:\phi{(X_3)}=[1,0,1,0,0],\:\phi{(X_4)=[0,1,0,1,0]}$。

我们可以找到$W=[-1,-1,-1,-1,6]$和$b=1$将上面的四个样本正确分类。

基于映射函数，我们又可以进一步改进优化目标为
$$
\min\limits_{W, b} \frac{1}{2}||W||^2+C\sum \limits_{i=1}^N\xi_i \\ s.t. \quad y_i(W^T\phi{(X_i)}+b)\ge 1-\xi_i \quad (i=1\sim N) \\ \xi_i \ge 0 \tag 4
$$
（4）式和（3）式的区别仅在将限制条件中$X_i$变为了$\phi{(X_i)}$，但需要注意的是，这只是显式的差别。事实上，在替换之后，因为$X_i$的维度升高到了$\phi{(X_i)}$的维度，因此$W$的维度也需要上升为同样。

**选择怎样的映射**

一个证明：在某个特征空间中随机地选取一些点，并且将这些点随机分到两个类别上。在越高维度的空间中，两类点被线性可分的概率越大。如果在一个无限维的空间中进行上述操作，则两类点线性可分的概率为1。

因此，如果可以的话，$\phi(X)$选择无限维会有更好的效果。但同时带来了一个问题，因为$W$的维度需要和$\phi(X)$保持一致，两个无限维的向量乘积无法计算。

#### 核函数

为了解决上述问题，SVM中创造性地提出了一种方法，称之为**核函数(Kernel Function)**。

**我们可以不知道无限维映射$\phi{(X)}$的显示表达，我们只要知道一个核函数$\kappa (X_1,X_2)=\phi(X_1)^T\phi(X_2)$，仍能使（4）这个优化式成立。**

这里只是引出核函数的概念，先不做详细证明。详细证明在后续的推导过程中。

**常用的核函数**

1. 高斯核$\displaystyle \kappa(X_1, X_2) = e^{-\frac{||X_1-X_2||^2}{2\delta^2}}=\phi(X_1)^T\phi(X_2)$，这里的$\phi(X)$是无限维。
2. 多项式核$\kappa(X_1, X_2) =(X_1^TX_2+1)^d$，其中$d$为多项式阶数。

**Mercer's Theorem**

核函数并不能随便选取，有以下定理：

对于一个核函数$\kappa(X_1,X_2)$，能写成$\phi(X_1)^T\phi(X_2)$的**充要条件**：

1. $\kappa(X_1,X_2) = \kappa(X_2,X_1)$（交换性）
2. $\forall C_i,X_i (i=1\sim N)$，有$\sum\limits^N_{i=1} \sum\limits^N_{j=1}C_iC_j\kappa(X_i,X_j)\ge 0$，（半正定性）

### 5.原问题与对偶问题

为了解决上述二次规划问题，我们需要引入一些优化理论中的知识。

**原问题（Prime Problem）**
$$
最小化：f(w) \\ 限制条件：g_i(w)\le0\quad (i=1\sim K) \\ h_i(w) = 0\quad(i=1\sim K)
$$
注意：这是一个非常普适的定义，正负号的选择可以统一最小化与最大化以及限制条件中的$\le$和$\ge$，而$h_i(w)$中包含一个$-c$的一次项即可统一值不为0的约束条件。

**对偶问题（Dual Problem）**

1. 定义：

$$
\begin{align}
L(w, \alpha, \beta) &= f(w) + \sum\limits^{K}_{i=1}\alpha_ig_i(w) + \sum\limits^{M}_{i=1}\beta_ih_i(w) \\ &=f(w) + \alpha^Tg(w) +\beta^Th(w)
\end{align}
$$

2. 对偶问题定义
   $$
   最大化：\theta(\alpha, \beta) = \inf\limits_{\forall w} L(w, \alpha, \beta)\\
   限制条件：\alpha_i \ge 0 \quad (i=1\sim N)
   $$
   解释：最大化的式子$\inf$表示，在确定了$\alpha$和$\beta$的情况下，遍历所有$w$，使$L(w, \alpha, \beta)$取最小值。而最大化则表示，对刚才的计算结果，遍历所有$\alpha与\beta$，求最大值。

**原问题与对偶问题的关系**

定理：**如果$w^*$是原问题的解，而$\alpha^*,\beta^*$是对偶问题的解，则有：$f(w^*) \ge \theta(\alpha^*,\beta^*)$。**

证明：
$$
\theta(\alpha^*,\beta^*)\quad = \quad\inf\limits_{所有w}{\{L(w, \alpha^*, \beta^*)\} \quad\le  \quad L(w^*,\alpha^*,\beta^*)} \quad=\quad f(w_*) + \sum\limits^{K}_{i=1}\alpha_i^*g_i(w^*) + \sum\limits^{M}_{i=1}\beta^*_ih_i(w^*) \tag {5.1}
$$
又因为$w^*$是原问题的解，所以满足$g_i(w^*)\le 0$，$h_i(w^*)=0$

又因为$\alpha^*,\beta^*$是对偶问题的解，所以满足$\alpha^*\ge0$。

由$g_i(w^*)\le 0$和$\alpha^*\ge0$，可知$ \sum\limits^{K}_{i=1}\alpha_i^*g_i(w^*) \le0$

由$h_i(w^*)=0$，可知$\sum\limits^{M}_{i=1}\beta^*_ih_i(w^*) = 0$

代入式5.1可知
$$
\theta(\alpha^*,\beta^*)\quad\le  \quad L(w^*,\alpha^*,\beta^*) \quad \le \quad f(w^*) \tag {5.2}
$$
**定义：$G=f(w^*)-\theta(\alpha^*,\beta^*) \ge 0$，$G$叫做原问题与对偶问题的间距（Duality Gap）。**

而对于某些特定的优化问题，可以证明对偶间距$G=0$。

**强对偶定理**

内容：若$f(w)$为凸函数，且$g(w)=Aw+b$，$h(w)=Cw+d$，则此优化问题的原问题与对偶问题的间距$G=0$，即$f(w^*)=\theta(\alpha^*,\beta^*)$。

**KKT条件**

如果强对偶定理成立，那么5.2式中的两个不等号都要取等。

考察5.1式中$\inf\limits_{所有w}{\{L(w, \alpha^*, \beta^*)\} \quad\le  \quad L(w^*,\alpha^*,\beta^*)} \quad=\quad f(w_*) + \sum\limits^{K}_{i=1}\alpha_i^*g_i(w^*) + \sum\limits^{M}_{i=1}\beta^*_ih_i(w^*)$

该式取等号意味着原问题的解$w^*$刚好是在$\alpha^*,\beta^*$值时，让$L(w, \alpha, \beta)$取到最小值的点。

而对于$f(w_*) + \sum\limits^{K}_{i=1}\alpha_i^*g_i(w^*) + \sum\limits^{M}_{i=1}\beta^*_ih_i(w^*) \quad \le \quad f(w^*)$

该式取等号意味着

对于$\forall i=1\sim K$，或者$ \alpha^*_i=0 $，或者$g_i^*(w^*)=0$

这个条件叫KKT条件

### 6.支持向量机的对偶问题求解

SVM优化目标
$$
\min\limits_{W, b} \frac{1}{2}||W||^2+C\sum \limits_{i=1}^N\xi_i \\ s.t. \quad y_i(W^T\phi{(X_i)}+b)\ge 1-\xi_i \quad (i=1\sim N) \\ \xi_i \ge 0 \tag 4
$$
原问题
$$
最小化：f(w) \\ 限制条件：g_i(w)\le0\quad (i=1\sim K) \\ h_i(w) = 0\quad(i=1\sim K)
$$
为了将SVM优化目标转化为原问题定义的形式，需要对（4）式进行简单的修改
$$
\min\limits_{W, b} \frac{1}{2}||W||^2-C\sum \limits_{i=1}^N\xi_i \\ s.t. \quad  1+\xi_i -y_iW^T\phi(X_i) - y_ib\le 0\quad (i=1\sim N) \\ \xi_i \le 0 \tag 5
$$
这样（5）式形式上与原问题定义相同。

转化为对偶问题
$$
最大化：\theta(\alpha, \beta) = \inf\limits_{所有(w,\xi_i,b)}\{\frac{1}{2}||W||^2-C\sum\limits^N_{i=1}\xi_i+\sum\limits^N_{i=1}\beta_i\xi_i+\sum\limits^N_{i=1}\alpha_i(1+\xi_i-y_iW^T\phi(X_i)-y_ib)\} \tag 6 \\
限制条件： \alpha_i \ge 0 \quad (i=1\sim N) \\
\beta_i \ge 0 \quad(i=1\sim N)
$$
要使$\inf$的式子值最小，需要令三个变量的偏导都为0，即
$$
\frac{\partial L}{\partial W} = W - \sum\limits^N_{i=1}\alpha_iy_i\phi(X_i) = 0 \\
\frac{\partial L}{\partial \xi_i} = -C+\beta_i+\alpha_i =  0\\
\frac{\partial L}{\partial b}= - \sum\limits^N_{i=1} \alpha_iy_i = 0
$$

整理一下可得
$$
W = \sum\limits^N_{i=1}\alpha_iy_i\phi(X_i) \\
\beta_i + \alpha_i = C \\
\sum\limits^N_{i=1}\alpha_iy_i = 0 \tag 7
$$
将（7）式代入（6）式中可得（8）（9）（10）
$$
\begin{align}
\frac{1}{2}||W||^2 & = \frac{1}{2}W^TW \\
& = \frac{1}{2}(\sum\limits^N_{i=1}\alpha_iy_i\phi(X_i))^T(\sum\limits^N_{j=1}\alpha_jy_j\phi(X_j)) \\
& = \frac{1}{2}\sum\limits^N_{i=1}\sum\limits^N_{j=1}\alpha_i\alpha_jy_iy_j\phi(X_i)^T\phi(X_j)
\end{align} \tag 8
$$

$$
\begin{align}
-\sum\limits^N_{i=1}\alpha_iy_iW^T\phi(X_i) &= -\sum\limits^N_{i=1}\alpha_iy_i(\sum\limits^N_{j=1}\alpha_jy_j\phi(X_j)^T)\phi(X_i) \\
&= -\sum\limits^N_{i=1}\sum\limits^N_{j=1}\alpha_i\alpha_jy_iy_j\phi(X_j)^T\phi(X_i)
\end{align} \tag 9
$$

$$
\begin{align}
\theta(\alpha, \beta) = \frac{1}{2}||W||^2 + \sum\limits^N_{i=1}\alpha_i - \sum\limits^N_{i=1}\alpha_iy_iW^T\phi(X_i) \tag {10}
\end{align}
$$

有意思的事情发生了，**在式子（8）(9）中仅有系数不同且其中均包含了核函数！**

进一步将（8）（9）式代入（10）可得
$$
\theta(\alpha, \beta) = \sum\limits^N_{i=1}\alpha_i - \frac{1}{2}\sum\limits^N_{i=1}\sum\limits^N_{j=1}\alpha_i\alpha_jy_iy_j\kappa(X_i, X_j) \tag {11}
$$
因此，最后需要求的问题就是
$$
最大化：\theta(\alpha) = \sum\limits^N_{i=1}\alpha_i-\frac{1}{2}\sum\limits^N_{i=1}\sum\limits^N_{j=1}\alpha_i\alpha_jy_iy_j\kappa(X_i,X_j) \\
限制条件： 0 \le \alpha_i \le C \\
\qquad\qquad\sum\limits^N_{i=1}\alpha_iy_i=0 \tag{12}
$$
这个问题也是一个凸优化问题，在这个问题中，我们未知的是$\alpha_i$，而这个问题有一个容易且有效的方法——**SMO算法**

我们这里暂时先不讲**SMO算法**，我们可以知道的是，通过SMO算法可以求得所有的$\alpha_i$。

但回到最开始，我们想要知道的是$W,b$所确定的超平面，怎样将$\alpha$转化为$W,b$

根据（7）式，$W = \sum\limits^N_{i=1}\alpha_iy_i\phi(X_i)$，好像仍需要知道$\phi(X_i)$的显式形式，没有解决这个问题，但事实上这是不需要的。

对于一个测试样例，我们认为

1. 若$W^T\phi(X)+b \ge 0$，则$y_{pred}=+1$

2. 若$W^T\phi(X)+b \lt 0$，则$y_{pred}=-1$

考察式子$W^T\phi(X)+b$，其中
$$
\begin{align}
W^T\phi(X) &= \sum\limits^N_{i=1}(\alpha_iy_i\phi(X_i))^T\phi(X) \\
&= \sum\limits^N_{i=1}\alpha_iy_i\kappa(X_i, X)
\end{align}
$$
其中又仅含有核函数了。

而对于$b$，我们则需要用到KKT条件求解。根据KKT条件定义和这个问题的形式，列出这个问题上的KKT条件
$$
\forall i=1\sim N,\\
1.\quad要么 \beta_i=0,要么\xi_i=0 \\
2.\quad 要么\alpha_i=0,要么1+\xi_i-y_iW^T\phi(X_i)-y_ib=0
$$
那么我们从所有求出的$\alpha$中取出一个满足$0 \lt \alpha_i \lt C$的

那么此时
$$
\beta_i \ne 0 \quad\Rightarrow \quad \xi_i = 0\\
\alpha_i \ne 0 \quad\Rightarrow \quad 1-y_iW^T\phi(X_i)-y_ib=0 \tag{13}
$$
通过（13）式可得
$$
b = \frac{1-y_i\sum\limits^N_{j=1}\alpha_jy_j\kappa(X_i, X_j)}{y_i}
$$
至此，所有问题都解决了。

## 三、算法的应用和具体实施方式

### 1.SVM算法整体方式

#### 训练流程

1. 输入训练样本$\{(X_i, y_i)\}_{i=1\sim N}$

2. 解优化问题
   $$
   最大化：\theta(\alpha) = \sum\limits^N_{i=1}\alpha_i - \frac{1}{2}\sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j\kappa(X_i, X_j) \\
   限制条件： 0\le \alpha_i \le C \\
   \qquad\qquad \sum\limits^N_{i=1}\alpha_i y_i = 0
   $$
   解这个优化问题的方法可以是SMO算法

3. 计算$b$

   找一个$0\lt \alpha_i \lt C$，则$\displaystyle b = \frac{1-y_i\sum\limits^N_{i=1}\alpha_iy_i\kappa(X_i,X_j)}{y_i}$

#### 测试流程

1. 输入测试样本$X$
2. 测试
   1. 若$\sum\limits^N_{i=1}\alpha_iy_i\kappa(X_i,X_j)+b\ge 0$，则$y_{pred} = +1$
   2. 若$\sum\limits^N_{i=1}\alpha_iy_i\kappa(X_i, X_j)+b\lt0$，则$y_{pred}=-1$

### 2.SMO算法

#### 伪代码

```python
target = desired output vector
point = training point matrix
    
def takeStep(i1, i2)
    if i1 == i2:
        return 0
    alph1 = Lagrange multiplier for i1
    y1 = target[i1]
    E1 = SVM output on point[i1] - y1
    s = y1 * y2
    Compute L, H
    if L == H:
        return 0
    k11 = kernel(point[i1], point[i1])
    k12 = kernel(point[i1], point[i2])
    k22 = kernel(point[i2], point[i2])
    eta = 2 * k12 - k11 - k22
    if eta < 0:
        a2 = alph2 - y2 * (E1 - E2) / eta
        if a2 < L:
            a2 = L
        else if a2 > H:
            a2 = H
    else:
        Lobj = objective function at a2=L
        Hobj = objective function at a2=H
        if Lobj > Hobj + eps:
            a2 = L
        else if Lobj < Hobj + eps:
            a2 = H
        else:
            a2 = alph2
    if |a2-alph2| < eps * (a2+alph2+eps):
        return 0
    a1 = alph1 + s * (alph2-a2)
    Update threshold to reflect change in Lagrange multipliers
    Update weight vector to reflect change in a1 & a2, if linear SVM
    Update error cache using new Lagrange multipliers
    Store a1 in the alpha array
    Store a2 in the alpha array
    return 1

def examineExample(i2):
    y2 = target[i2]
    alph2 = Lagrange multiplier for i2
    E2 = SVM output on point[i2] - y2
    r2 = E2 * y2
    if (r2 < -tol and alph2 < C) or (r2 > tol and alph2 > 0):
        if (number of non-zero and non-C alpha > 1):
            i1 = result of second choice heuristic
            if takeStep(i1, i2):
                return 1
        loop over non-zero and non-C alpha, starting at random point:
            i1 = identity of current alpha
            if takeStep(i1, i2):
                return 1
        loop over all possible i1, starting at a random point:
            i1 = loop variable
            if takeStep(i1, i2):
                return 1
    return 0

def main():
    initialize alpha array to all zero
    initialize threshold to zero
    numChanged = 0
    examineAll = 1
    while numChanged > 0 or examineAll:
        numChanged = 0
        if examineAll:
            loop I over all training examples:
                numChanged += examineExample(I)
        else:
            loop I over examples where alpha is not 0 and not C:
                numChanged += examineExample(I)
        if examineAll == 1:
            examineAll = 0
        else if numChanged == 0:
            examineAll = 1

if __name__ == '__main__':
    main()
```

### 2.算法的应用

#### 线性可分数据集

构造出一个线性可分的二分类数据集

![image-20220525182417324](C:\Users\xieyucheng\AppData\Roaming\Typora\typora-user-images\image-20220525182417324.png)

共有98个样本点

使用SVM对样本集进行分类预测

```python
import matplotlib.pyplot as plt
import numpy as np
from sklearn import svm
 
def loadDataSet(fileName):
    dataMat = []
    labelMat = []
    fr = open(fileName)
    for line in fr.readlines():
        lineArr = line.strip().split('\t')
        dataMat.append([float(lineArr[0]), float(lineArr[1])])
        labelMat.append(float(lineArr[2]))
    return dataMat, labelMat

X, Y = loadDataSet('./perceptron_data.txt')
X = np.mat(X)
 
#训练模型
clf = svm.SVC(C=10,kernel='linear',gamma='auto')
clf.fit(X, Y)
```

对训练的样本集以及所得的划分超平面进行可视化

```python
# 获取分割超平面
w = clf.coef_[0]

xx = np.linspace(-5, 5)
yy = -w[0]/w[1] * xx - (clf.intercept_[0]) / w[1] # clf.intercept_[0]截距
 

# 通过支持向量绘制分割超平面
print("support_vectors_=", clf.support_vectors_)
b = clf.support_vectors_[0]
 
yy_down = -w[0]/w[1] * xx + (b[1] - -w[0]/w[1] * b[0])
b = clf.support_vectors_[-1]
yy_up = -w[0]/w[1] * xx + (b[1] - -w[0]/w[1] * b[0])

# 画出直线，散点以及临界点的支持向量平面
plt.plot(xx, yy, 'k-')
plt.plot(xx, yy_down, 'k--')
plt.plot(xx, yy_up, 'k--')
 
plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80, facecolors='none')
plt.scatter(X[:, 0].flat, X[:, 1].flat, c=Y, cmap=plt.cm.Paired)
plt.axis('tight')
plt.show()
```

得到的结果

<img src="https://te4p0t.github.io/assets/images/typora-user-images/202206141101864.png" alt="image-20220525182659577" style="zoom:67%;" />

#### 兵王问题

**问题描述**

国际象棋的残局中，黑方只剩下一个王，白方剩一个兵和一个王。结局只有两种：白方将死黑方获胜，和棋。

**数据集**

![image-20220525200500722](C:\Users\xieyucheng\AppData\Roaming\Typora\typora-user-images\image-20220525200500722.png)

前6列代表剩余3个子的坐标，第7列代表胜负，其中“draw”表示和棋，英文字母代表白方需要几步能够将死黑方，例：“eight”代表需要8步

**代码**

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

# 读取数据
data = pd.read_csv('./krkopt.data', header=None)

def svm_c(x_train, x_test, y_train, y_test):
    svc = SVC(C=8, kernel='rbf', class_weight='balanced',gamma='auto')
    c_range = np.logspace(-5, 15, 2, base=2)
    gamma_range = np.logspace(-9, 3, 2, base=2)
    # 网格搜索交叉验证的参数范围，cv=3,3折交叉
    param_grid = [{'kernel': ['rbf'], 'C': c_range, 'gamma': gamma_range}]
    grid = GridSearchCV(svc, param_grid, cv=3, n_jobs=-1)
    # 训练模型
    clf = grid.fit(x_train, y_train)
    acc = grid.score(x_test, y_test)
    print('精度为%s' % acc)
    

# 样本数值化 a,b,c...h 转化为 1,2,3...8
for i in [0, 2, 4]:
    data.loc[data[i] == 'a', i] = 1
    data.loc[data[i] == 'b', i] = 2
    data.loc[data[i] == 'c', i] = 3
    data.loc[data[i] == 'd', i] = 4
    data.loc[data[i] == 'e', i] = 5
    data.loc[data[i] == 'f', i] = 6
    data.loc[data[i] == 'g', i] = 7
    data.loc[data[i] == 'h', i] = 8

data.loc[data[6] == 'draw', 6] = -1
data.loc[data[6] == 'zero', 6] = 0
data.loc[data[6] == 'one', 6] = 1
data.loc[data[6] == 'two', 6] = 2
data.loc[data[6] == 'three', 6] = 3
data.loc[data[6] == 'four', 6] = 4
data.loc[data[6] == 'five', 6] = 5
data.loc[data[6] == 'six', 6] = 6
data.loc[data[6] == 'seven', 6] = 7
data.loc[data[6] == 'eight', 6] = 8
data.loc[data[6] == 'nine', 6] = 9
data.loc[data[6] == 'ten', 6] = 10
data.loc[data[6] == 'eleven', 6] = 11
data.loc[data[6] == 'twelve', 6] = 12
data.loc[data[6] == 'thirteen', 6] = 13
data.loc[data[6] == 'fourteen', 6] = 14
data.loc[data[6] == 'fifteen', 6] = 15
data.loc[data[6] == 'sixteen', 6] = 16

# 归一化处理
for i in range(6):
    data[i] = (data[i]-data[i].mean())/data[i].std()

# 拆分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, :6], data[6].astype(int), test_size=5000/len(data))
svm_c(X_train, X_test, y_train, y_test)
```

使用1000个样本进行训练，最终准确率为0.828

![image-20220525203312988](C:\Users\xieyucheng\AppData\Roaming\Typora\typora-user-images\image-20220525203312988.png)

#### 手写数字识别

使用经典数据集MNIST，用SVM训练测试效果

```python
from os import access
from sklearn import svm
import numpy as np
from tensorflow.examples.tutorials.mnist import input_data
import matplotlib.pyplot as plt

# 利用tensorflow中的数据
mnist = input_data.read_data_sets('MNIST_data', one_hot=False)

x_train = mnist.train.images
y_train = mnist.train.labels
x_test = mnist.test.images
y_test = mnist.test.labels

train_num = 5000
test_num = 1000
print("train_num:", train_num)

# 设置SVM训练参数
clf = svm.SVC(C=10,kernel='rbf',gamma='auto')
clf.fit(x_train[:train_num], y_train[:train_num])

# 计算精度
result = clf.predict(x_test[:test_num])
acc = np.sum(np.equal(result, y_test[:test_num])) / test_num
print("test_num:", test_num)
print(acc)

# 随机选取20张图片进行可视化
select = np.random.randint(0, len(x_test), 20)
test = []
im = []
true = []
pred = []
for i in range(len(select)):
    im.append(x_test[select[i]].reshape(28, 28))
    true.append(y_test[select[i]])
    test.append(x_test[select[i]])
pred = clf.predict(test)
fig, axarr = plt.subplots(int(len(im)/5),5,figsize=(12,120))
print(axarr)
for i in range(len(im)):
    axarr[int(i/5)][i%5].axis('off')
    axarr[int(i/5)][i%5].imshow(im[i], cmap='gray', aspect='auto')
    axarr[int(i/5)][i%5].set_title("label:{}, pred:{}".format(true[i], pred[i]), y=-0.2)
plt.show()
```

结果

选取5000张训练图片，1000张测试图片，测试结果准确率为0.91

<img src="https://te4p0t.github.io/assets/images/typora-user-images/202206141101602.png" alt="image-20220525194304351" style="zoom:67%;" />

## 四、总结与展望

### 总结

SVM算法是一个数学上及其优美的算法，有着严密的数学推导和清晰的结构。同时因为SVM的决策面只依赖于支持向量，并且选择的决策面具有很好的泛化性能，因此在小样本问题上有着极大的优势。

它在设计的时候有很多很有意思的创新点。

1. 其他的分类器在面对线性不可分的问题时，更多地是去寻找一种拟合非线性划分超平面的方法。例如神经网络引入非线性激活函数、决策树拟合多段斜线来对复杂分类边界进行近似等等。而SVM采用了不同的方法，他通过一个低维到高维的映射，将样本空间映射到高维，在高维中寻找一个可能的线性划分超平面。
2. SVM引入了核方法，理论上能将样本空间投影到无限维，增强了SVM的分类性能。同时，对于无限维的投影函数，又不需要显式地知道其形式并计算，而是通过核函数巧妙地避开了这个难题。由于是在高维特征空间中建立的线性学习机，所以与线性模型相比，几乎不增加计算复杂性，而且避免了“维数灾难”的问题。

### 展望

SVM虽然有着上述种种优点，但也存在着一些问题。如：

1. SVM在面对大规模数据时，效率较低。对于线性核的SVM有许多提高效率的成果。但非线性核SVM的时间复杂度在理论上不可能低于$O(m^2)$，因此只能研究设计快速近似算法。
2. SVM是针对二分类任务设计的，对多分类任务要进行专门的推广。
3. 核函数的选择直接决定了SVM的最终性能。但目前对核函数的选择仍是一个未解决的问题，只能通过经验或尝试来确定。多核学习使用多个核函数并通过学习获得其最优凸组合来作为最终的核函数，这实际上是在借助集成学习机制。

对于这些问题的解决和改进仍是值得研究的问题。